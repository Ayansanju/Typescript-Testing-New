# Hallucination

Hallucination in LLMs refers to generate plausible-sounding but factually incorrect or fabricated information. This seems that it generate the answer which is looking good and sounds good but actually it is totally wrong. Forexample you asked "Who is the Prime Minister of England ?" and LLM give the answer "Mr John". It sounds okay but actually it is tollay wrong. This occurs when models fill knowledge gaps or present uncertain information with apparent certainty. Mitigation techniques include requesting sources, asking for confidence levels, providing context, and always verifying critical information independently (Cross check or verification).
